#\file spark.howto: Big Data - Data Science notes and facts.
       		    	       Apache Spark tutorial.




#===============================================================================
# INDEX
#===============================================================================

1&2) Lecture 1&2: Introduction.- What are Big Data and Data science

2) Lecture 3: Big Data, hardware, Map-Reduce vs Apache Spark

3) Lecture 3: Big Data & Cluster Computing: Map-Reduce vs Apache Spark

4) Lecture 4: Spark Essentials
   4.1) RDDs
   4.2) Spark Program Lifecycle
   	4.2.1) Creating an RDD
	4.2.2) Spark Transformations
	4.2.3) Caching RDDs and storage options
	       4.2.3.1) Caching RDDs
	       4.2.3.2) Unpersist and storage options
	4.2.4) Spark Actions
   4.3) Spark Key-Value RDDs
   	4.3.1) Key-Value Transformations
   4.4) pySpark Closures
   4.5) pySpark Shared Variables
   	4.5.1) Broadcast Variables
   	4.5.2) Accumulators
   4.6) Operation's Tips & Tricks

5) Lecture 5: Semi-Structured Data
   5.1) The Structure Spectrum
   5.2) Semi-Structured Tabular Data
   5.3) Pandas: Python Data Analysis Library 
   	5.3.1) Pandas DataFrame
   5.4) pySpark DataFrames: Semi-Structured Data in pySpark
   5.5) Semi-Structured Log Files
   5.6) Splunk: Data Mining ("Splunking")
   5.7) File Performance - Summary

6) Lecture 6: Structured data - Relational Database
   6.1) Definitions
   6.2) The Structured Query Language (SQL)
   6.3) pyspark.sql and pySpark Joins
   	6.3.1) Pair RDD Joins

7) Lecture 7: Data Quality
   7.1) Data Cleaning
   7.2) Data Quality: Problems, Sources, and Continuum
   7.3) Data Gathering, Delivery, Storage, Retrieval, Mining/Analysis
   7.4) Data Quality Constraints and Metrics
   7.5) Data Integration

8) Lecture 8: Exploratory Data Analysis and Machine Learning
   8.1) Descriptive Statistics 
   	8.1.1) Exploratory Data Analysis [6]
   8.2) Inferential  Statistics
   	8.2.1) Machine learning techniques
   	8.2.2) Spark’s Machine Learning Toolkit   

Annex A) spark_tutorial_student

References)


#===============================================================================
#===============================================================================


CONTENT


#===============================================================================
# 1&2) Lecture 1&2: Introduction.- What are Big Data and Data science
#===============================================================================

def) Big Data: data sets with sizes beyond the ability of commonly used
     software tools to properly handle it within a tolerable elapsed time =>
     data science to manage big data.

     - Big Data vs Data analytic: Volume, Velocity, Variety.


def) Data Science aims to derive knowledge from big data, efficiently and
     intelligently"


def) Data science encompasses the set of activities, tools, and methods that
     enable data-driven activities in science, business, medicine, ...


def) Data science = mix (hacking skills, domain expertise, math & stats know.)


1.1) Data Science vs Other systems/techniques

-  Data Science vs Databases

   Timing:       Querying the future  / Querying the past
   Data      :   massive & cheap      / modest volume & "precious" value
   Priorities:   Speed, availability  / consistency, error recovery, auditable
   Structured:   weak or none (test)  / strongly (schema)
   Realizations:      no SQL          / SQL 
   Properties:   CAP*, eventual cons. / Transactions, ACID+

   * CAP:  Consistency, Availability, Partition Tolerance
   + ACID: Atomicity, Consistency, Isolation, Durability


- Scientific Computing (Modeling) vs Data Science (Data-Driven Approach)

  Physics-based MODELS         / General INFERENCE ENGINE replaces model
  Problem-Structured           / Structure not related to problem
  Mostly DETERMINISTIC precise / Statistical models handle TRUE RANDOMNESS,
  	 	       	         and unmodeled complexity
  Run on SUPERCOMPUTER or      / Run on CHEAPER computer Clusters (EC2)
  Computing Cluster"


- Traditional Machine Learning vs Data Science

  Develop new (individual) models  / Explore many models, build and tune hybrids
  Prove math. properties of models / Understand empirical properties of models
  Improve/validate on a few,       / Develop/use tools that can handle MASSIVE
  relatively CLEAN, SMALL DATASETS   datasets
  Publish a paper                  / TAKE ACTION!


1.2) Data Science Topics

• Data Acquisition (where is the data?)
• Data Preparation (get the proper input data format)
• Analysis
• Data Presentation
• Data Products
• Observation and Experimentation


1.3) Data Acquisition & Data Preparation

1.3.1 Overview

• ETL: Extract, Transform, Load
  » We need to extract data from the source(s)
  » We need to load data into the sink
  » We need to transform data at the source, sink, or in a staging area

- Sources: files (csv, excel, logs), databases, web site, ...
- Sinks: code files (Python, R, ...), DB manage systems, ...

1.3.2) Process Model

• The construction of a new data preparation process is done in many phases:
  » Data characterization
  » Data cleaning
  » Data integration

• We must efficiently move data around in space and time
  » Data transfer
  » Data serialization and deserialization (for files or network)

WARNING Impediments to Collaboration:
  » DIVERSITY of tools (sources & sinks)
  » No/bad documentation and version controlling.




#===============================================================================
# 3) Lecture 3: Big Data & Cluster Computing: Map-Reduce vs Apache Spark
#===============================================================================

- The Big Data Problem
  • A single machine can no longer process or even store all the data!
  • Only solution is to DISTRIBUTE DATA over large clusters


3.1) Map Reduce: disk => MAP => disk => REDUCE => disk

     PROBLEM: disk I/O is very slow!


3.2) Spark (Distributed execution engine)

    SOLUTION: In-Memory Data sharing (use memory instead of disk)


3.2.1) Spark Core and Resilient Distributed Datasets (RDDs)

- Spark Core: provides distributed task dispatching, scheduling, and basic I/O.

  • Provides PROGRAMMING ABSTRACTION and PARALLEL RUNTIME to hide complexities
    of fault-tolerance and slow machines.

  • “Here’s an operation, run it on all of the data”
    » I don’t care where it runs (you schedule that)
    » In fact, feel free to run it twice on different nodes


- Resilient (*1) Distributed Datasets (RDDs):

  • It is the fundamental programming abstraction, A LOGICAL COLLECTION OF DATA
    PARTITIONED ACROSS MACHINES, stored in memory or in disk.
  
  • RDDs can be created by referencing datasets in external storage systems, or
    by applying a diverse set of parallel transformations (map, filter, join)
    and actions (count, collect, save).

  • The RDD abstraction is exposed through a language-integrated API ->
    -> manipulate RDDs is similar to manipulating local collections of data.

  • RDDs automatically rebuilt on machine failure"


  (*1) resilient = elástico


- Spark Tools
  • Spark SQL
  • Spark Streaming
  • MLlib (machine learning)
  • GraphX (graph)
  

- Spark and Map Reduce Differences 

  	    	Hadoop			Spark
	    	Map Reduce
________________________________________________________________________
             |                    |
Storage      |	Disk only	  |	In-memory or on disk
             |                    |
Operations   |	Map and Reduce	  |	Map, Reduce, Join, Sample, etc…
             |                    |
Execution    |	Batch   	  |	Batch, interactive, streaming
model	     |                    |
             |                    |
Programming  |	Java		  |	Scala, Java, R, and Python
environments |                    |


- Other Spark advantages

  • Generalized patterns -> unified engine for many use cases

  • Lazy evaluation of the lineage graph -> reduces wait states, best pipelining

  • Lower overhead for starting jobs

  • Less expensive shuffles




#===============================================================================
# 4) Lecture 4: Spark Essentials
#===============================================================================

- pySpark: python spark API


- Spark program = 2 programs = 1 driver program + n worker programs

  The driver has Spark jobs that it needs to run and these jobs are split into
  tasks that are submitted to the executors for completion. The results from
  these tasks are delivered back to the driver.

  i) Assigning a job (driver)

     job (stage): a stage is a group of tasks that all perform the same
     	 	  computation, but on different input data

     1 job (stage) -> n tasks
     1 task -> executed in 1 partition within an executor (worker)

  ii) Resolving tasks in parallel (workers)

  iii) Resolving the job

      n tasks results delivered back to the driver
      driver combine the results

      WARNING the data returned to the driver must fit into the driver's
      	      available memory. If not, the driver will crash.


- Spark driver program (SparkContext Object)

  • Runs the user’s main function and executes various parallel operations

  • Runs on a (UNIQUE) CLUSTER.


- Worker programs (executor Object): 

  • Run in cluster nodes or in local threads.

  • RDDs are distributed across workers


- class pyspark.SparkContext()

  • A spark program first creates a "SparkContext object"

  • Represents the connection to a Spark cluster: tells Spark how and where to
    access a cluster.

  • Can be used to create RDDs and broadcast variables on that cluster.

  • The "Master" parameter determines which type and size of cluster to use:
    local, local[k], spark cluster, mesos cluster.


INFO pySpark shell and Databricks Cloud automatically create the "sc" variable

INFO iPython and programs must use a constructor to create a new SparkContext


4.1) RDDS  [3]#pyspark.RDD

     RDD = logical collection of data partitioned across machines:

    - In Spark, datasets are represented as a list of entries

    - the list is broken up into many different partitions-

    - Each partition stored on a different machine.

    - Each partition holds a UNIQUE subset of the entries in the list.

    - Partitions are each stored in a worker's memory


  » Immutable once constructed

  » Track lineage information to efficiently recompute lost data

  » Enable OPERATIONS on collection of elements IN PARALLEL

- How to create an RDD:
  i)   by PARALLELIZING existing Python collections (lists)
  ii)  by TRANSFORMING an existing RDDs
  iii) FROM FILES in HDFS or any other storage system

- The programmer specifies number of partitions for an RDD: more partitions =
  more parallelism.

  WARNING  #partitions !=  #workers  (e.g. 5 partitions can run in 3 workers)


4.2) Spark Program Lifecycle

  i)   Create an RDD
  ii)  Apply transformations to an RDD (lazy eval.)
  iii) cache() some RDDs for reuse
  iv)  Apply actions to an RDD


WARNING What code is executed in what program. Summary:

 • RDDs creation are executed (in parallel) in the workers.

 • Transformations are executed (in parallel) in the workers.

 • Actions are executed (in parallel) in the workers

 • Actions result-combination is executed in the driver.


4.2.1) Creating an RDD

WARNING lazy evaluation -> Spark only records what to create.
 
a) From Python collections(lists)

  parallelize(collection, numSlices=None) [3].pyspark

INFO Using xrange is recommended if the input represents a range for performance

>>> data = [1,2,3,4,5]
>>> rdd = sc.parallelize(xra, 4) # RDD distributed in 4 partitions

>>> rdd.id() # each RDD gets a unique ID [1, 2, ...]


b) From files (text, HDFS, ...)

  textFile(name, minPartitions=None, use_unicode=True) [3].pyspark

• Read a text file from HDFS, a local file system (available on all nodes), or
  any Hadoop-supported file system URI, and return it as an RDD of Strings.

• Elements are lines of input.

>>> distFile = sc.textFile("README.md", 4) 


4.2.2) Spark Transformations [3]

WARNING RDDs inmutables => each transformation actually creates a new RDD.

• Create new datasets from an existing one

• Transformations specify how to perform parallel computation in a LAZILY
  EVALUATED manner.

INFO Spark optimizes the required calculations

INFO Spark recovers from failures and slow workers


WARNING Function literals (lambda) are closures. Spark automatically pushes
	closures to workers.

INFO You can see the set of transformations that were applied to create an RDD
     by using the toDebugString() method: print myrdd.toDebugString()
  

- map(func): func(input) | dataset

  >>> rdd = sc.parallelize([1, 2, 3, 4])
  >>> rdd.map(lambda x: x * 2)
  RDD: [1, 2, 3, 4] -> [2, 4, 6, 8]


- filter(func):  func(input)==true | dataset

  >>> rdd = sc.parallelize([1, 2, 3, 4])
  >>> rdd.filter(lambda x: x % 2 == 0)
  RDD: [1, 2, 3, 4] -> [2, 4]


- distinct([numTasks]): distinct(input) | dataset

  >>> rdd = sc.parallelize([1, 4, 2, 2, 3, 4])
  >>> rdd.distinct()
  RDD: [1, 4, 2, 2, 3, 4] -> [1, 4, 2, 3]


- flatMap(func): func(input) | dataset

  >>> rdd = sc.parallelize([1, 2, 3, 2])

  >>> rdd.map(lambda x: [x, x+5])
  RDD: [1, 2, 3, 2] -> [[1,6], [2,7], [3,8], [2,7]]

  >>> rdd.flatMap(lambda x: [x, x+5])
  RDD: [1, 2, 3, 2] -> [1, 6, 2, 7, 3, 8, 2, 7]


-  mapValues(f)
   
   Pass each value in the key-value pair RDD through a map function without
   changing the keys; this also retains the original RDD’s partitioning.

    >>> x = sc.parallelize([("a", ["apple", "banana", "lemon"]), 
                           ("b", ["grapes"])])
    >>> x.mapValues(len).collect()
    [('a', 3), ('b', 1)]


    WARNING usefull to count (group) different values.


-  flatMapValues(f)


- mapPartitions(f, preservesPartitioning=False): 

  Return a new RDD by applying a function to each partition of this RDD.

    >>> rdd = sc.parallelize([1, 2, 3, 4], 2)
    >>> def f(iterator): yield sum(iterator)
    >>> rdd.mapPartitions(f).collect()
    [3, 7] # partition 1, sum[1,2]; partition 2, sum [3,4] 


- mapPartitionsWithIndex(f, preservesPartitioning=False): 

  Return a new RDD by applying a function to each partition of this RDD, while
  tracking the index of the original partition. For every partition (index,
  iterator) pair, the function returns a tuple of the same partition index
  number and an iterator of the transformed items in that partition.

    >>> rdd = sc.parallelize([1, 2, 3, 4], 4)
    >>> def f(splitIndex, iterator): yield splitIndex
    >>> rdd.mapPartitionsWithIndex(f).sum()
    6

- union(other)

    Return the union of this RDD and another one.

    >>> rdd = sc.parallelize([1, 1, 2, 3])
    >>> rdd.union(rdd).collect()
    [1, 1, 2, 3, 1, 1, 2, 3]


    ...

    see [3]

    ...

4.2.3) Caching RDDs and storage options 

4.2.3.1) Caching RDDs

INFO For efficiency Spark keeps your RDDs in memory. By keeping the contents in
     memory, Spark can quickly access the data. However, memory is limited, so
     if you try to keep too many RDDs in memory, Spark will automatically
     delete RDDs from memory to make space for new RDDs. If you later refer to
     one of the RDDs, Spark will automatically recreate the RDD for you, but
     that takes time.  So, if you plan to use an RDD more than once, then you
     should tell Spark to cache that RDD.  

     However, if you cache too many RDDs and Spark runs out of memory, it will
     delete the least recently used (LRU) RDD first. Again, the RDD will be
     automatically recreated when accessed.


- cache(): Persist this RDD with the default storage level (MEMORY_ONLY_SER).

  >>> lines = sc.textfile("...", 4)
  >>> lines.cache() # save, don't recompute inside the print()
  >>> comments = lines.filter(iscomment)
  >>> print (lines.count(), comments.count())

- is_cached: check if an RDD is cached 

  >>> print(lines.is_cached)
  True


4.2.3.2) Unpersist and storage options

INFO Spark automatically manages the RDDs cached in memory and will save them
     to disk if it runs out of memory. For efficiency, once you are finished
     using an RDD, you can optionally tell Spark to stop caching it in memory
     by using the RDD's unpersist() method to inform Spark that you no longer
     need the RDD in memory.

- unpersist(): Mark the RDD as non-persistent, and remove all blocks for it
  	       from memory AND DISK.


INFO you can directly query the current storage information for an RDD using
     the getStorageLevel() operation.

  filteredRDD.unpersist()
  print "Storage level for a non cached RDD: ", filteredRDD.getStorageLevel()
  filteredRDD.cache()
  print "Storage level for a cached RDD:     ", filteredRDD.getStorageLevel()
  #
  # Result:
  # Storage level for a non cached RDD:  Serialized 1x Replicated
  # Storage level for a cached RDD:      MEMORY Serialized 1x Replicated


- persist(storageLevel=StorageLevel(False, True, False, False, 1))

  Set this RDD’s storage level to persist its values across operations after
  the first time it is computed.



- join(other, numPartitions=None)

  Return an RDD containing all pairs of elements with matching keys in self and
  other. Performs a hash join across the cluster.

    >>> x = sc.parallelize([("a", 1), ("b", 4)])
    >>> y = sc.parallelize([("a", 2), ("a", 3)])
    >>> sorted(x.join(y).collect())
    [('a', (1, 2)), ('a', (1, 3))]



4.2.4) Spark Actions

• Actions cause the transformations to be executed.  

• Mechanism for getting results out of Spark.


- reduce(func)

  • aggregate dataset’s elements using function func.

  • func takes two arguments and returns one.

  • func MUST BE commutative and associative so that it can be computed
    correctly in paralle. Otherwise, the results from reduce() will be
    inconsistent.

  >>> rdd = sc.parallelize([1, 2, 3])
  >>> rdd.reduce(lambda a,b: a * b)    # causes parallelize to be executed
  Value: 6  # (1*2*3)


- take(n): return an array with the first n elements of the RDD

  >>> rdd.take(2)
  Value: [1, 2]


- first(): return first element == take(1)


- collect(): return a list that contains ALL of the elements in this RDD.

  >>> rdd.collect()
  Value: [1,2,3]

  WARNING make sure will fit in driver program
  • Forum note.- https://piazza.com/class/i9esrcg0gpf8k?cid=799
  • Forum note (Anthony D. Joseph).- You can see the sizes of RDDs and worker
    memory in the Spark UI under the Executors tab.


- count(): Return the number of elements in this RDD

  >>> rdd.count()
  Value: 3


- takeOrdered(n, key=None): return n elements ordered in ascending order or as
  		 	    specified by the optional key function.

  >>> rdd = sc.parallelize([5, 3, 1, 2])
  >>> rdd.takeOrdered(3)
  Value: [1,2,3]  
  >>> rdd.takeOrdered(3, lambda s: -1 * s)
  Value: [5,3,2]



- top(num, key=None): Get the top N elements from a RDD.

  Note: It returns the list sorted in descending order. (inverse to takeOrdered)

  >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)
  [12]
  >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)
  [6, 5]
  >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)
  [4, 3, 2]


- takeSample(withReplacement, num, seed=None): return a fixed-size sampled
   subset of this RDD.



- countByKey(): Count the number of elements for each key, and return the
  result to the master as a dictionary. Value can be any data type (not only
  numbers)

    >>> rdd = sc.parallelize([("a", "hola"), ("b", 222.5), ("a", "que hase")])
    >>> sorted(rdd.countByKey().items())
    [('a', 2), ('b', 1)]



- countByValue(): Return the count of each unique value in this RDD as a
  dictionary of (value, count) pairs.

  >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())
  [(1, 2), (2, 3)] # value 1: twice; value 2: three times


4.3) Spark Key-Value RDDs

• Similar to Map Reduce, Spark supports Key-Value pairs

• Each element of a Pair RDD is a pair tuple

  >>> rdd = sc.parallelize([(1,2),(3,4)])
  RDD:[(1,2),(3,4)]


4.3.1) Key-Value Transformations

- reduceByKey(func, numPartitions=None): func(input) | dataset of (K,V) pairs

  • func must be of type (V,V) -> V
  • Output will be hash-partitioned with numPartitions partitions

  >>> from operator import add
  >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])

  >>> sorted(rdd.reduceByKey(add).collect())
  # OR
  >>> sorted(rdd.reduceByKey(lambda a,b: a+b)).collect())
  RDD: [("a", 1), ("b", 1), ("a", 1)] -> [('a', 2), ('b', 1)]
  

- sortByKey(): return a new dataset (K, V) pairs sorted by keys in ascending
  	       order.

  >>> rdd = sc.parallelize([(1, 'a'), (2, 'c'), (1, 'b')])
  >>> rdd.sortByKey()
  RDD: [(1, 'a'), (2, 'c'), (1, 'b')] -> [(1, 'a'), (1, 'b'), (2, 'c')]
  


- groupByKey(numPartitions=None)

    Group the values for each key in the RDD into a single
    sequence. Hash-partitions the resulting RDD with numPartitions partitions.

    
    # list(values)
    >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])
    >>> sorted(rdd.groupByKey().mapValues(len).collect())
    [('a', 2), ('b', 1)]

    # len(values)
    >>> sorted(rdd.groupByKey().mapValues(list).collect())
    [('a', [1, 1]), ('b', [1])]

 
WARNING groupByKey() can cause a lot of data movement across the network and
  	create large Iterables at workers.

WARNING If you are grouping in order to perform an aggregation (such as a sum
    	or average) over each key, using reduceByKey or aggregateByKey,
    	(combineByKey, foldByKey) will provide much better performance.



- combineByKey() can be used when you are combining elements but your return
  type differs from your input value type.

  >>> x = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])
  >>> def f(x): return x
  >>> def add(a, b): return a + str(b)
  >>> sorted(x.combineByKey(str, add, add).collect())
  [('a', '11'), ('b', '1')]


- foldByKey() merges the values for each key using an associative function and
  a neutral "zero value".

  >>> print "foldByKey value=str: "
  >>> rdd = sc.parallelize([("a", "hola"), ("b", "ddd"), ("a", "que hase")])
  >>> rddfold = rdd.foldByKey("",add)
  >>> print rddfold.collect()
  [('a', 'holaque hase'), ('b', 'ddd')]


  >>> print "foldByKey value=integer: "
  >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])
  >>> print sorted(rdd.foldByKey(0, add).collect())
  [('a', 2), ('b', 1)]


4.4) pySpark Closures  (driver -> worker)

• Spark automatically creates closures for:
  » Functions that run on RDDs at workers
  » Any global variables used by those workers

• One closure per worker
  » Sent for every task
  » No communication between workers
  » Changes to global variables at workers are not sent to driver

Problems:
  » Closures are (re-)sent with every job
  » Inefficient to send large data to each worker
  » Closures are one way: driver -> worker

Solution: pySpark Shared Variables


4.5) pySpark Shared Variables

4.5.1) Broadcast Variables (driver -> worker)

  » SHIP to each worker ONLY ONCE instead of with each task
  » On workers: read-only, cached.

  - Creation (driver)   variable = sc.broadcast()
  - Access (workers)    variable.value

  >>> broadvar = sc.broadcast([1,2,3])  # driver
  >>> broadvar.value  # At the worker (in code passed via closure)
  [1, 2, 3]

  >>> idfsSmallWeights = idfsSmall.collectAsMap()
  >>> idfsSmallBroadcast = sc.broadcast(idfsSmallWeights)


4.5.2) Accumulators  (worker -> driver)

       - accumulator(value, accum_param=None): create & init an accumulator

       - value: get the accumulator value (driver)

       - add(term): commutative and associative “add” operation (worker)


• Tasks at workers cannot access accumulator’s values (write-only)

• Used to: Aggregate values from workers back to driver (e.g. PARALLEL counters
       	   and sums)

• Accumulators can be used in actions or transformations:  
  » Actions: each task’s update to accumulator is applied only once
  WARNING Transformations: no guarantees (use only for debugging)

• Types
  » SparkContext: integers, double, long, float
  » User defined: via AccumulatorParam object (See lab for example of custom
    type)


• (e.g. counting empty lines)

  file = sc.textFile(input_file)   # Create RDD
  blank_lines = sc.accumulator(0)  # Create & init Accumulator[Int]
  
  # def a function that use the accumulator inside
  def extract_call_signs(line)
      global blank_lines  # make accessible the global accumulator variable 
      if (line==""):
      	 blank_lines+=1   # or 'add(blank_lines)'
      return line.split(" ")

  call_signs = file.flatMap(extract_call_signs)
  print ("blank lines: %d" % blank_lines.value)  # (driver) get value


4.6) Operation's Tips & Tricks

a) How to refer to collections fields in lambda functions

   collection = ('id', value)

   lambda x: x[1] > 10

b) Order a MAP pairs(key,value) by value (a use example of a) above) 

  >>>rdd.parallelize([('rat', 2), ('elephant', 1), ('cat', 43)], )
  >>>rdd.takeOrdered(2, lambda x: -x[1]) # -x == descending order
  cat: 43
  rat: 2


#===============================================================================
# 5) Lecture 5: Semi-Structured Data
#===============================================================================

5.1) The Structure Spectrum

     • Structured data (schema-first)
     • Semi-Structured data (schema-later)
     • Unstructured (schema-never)

- data model: collection of concepts for describing data

- schema: description of a particular collection of data, using a given data
  	  model.

5.2) Semi-Structured Tabular Data

- Table: collection of rows and columns

- Problems: 
  • Format not well defined => data missing, data inconsistency, ...
  • Multiple source of data (csv files, text files, sensors ...)


5.3) Pandas: Python Data Analysis Library 

     INFO An alternative to R

5.3.1) Pandas DataFrame

  - A table with named columns
    
  - Represented as a Python dict (column_name -> Series object)

  - R has a similar data frame type


5.4) pySpark DataFrames: Semi-Structured Data in pySpark

     class pyspark.sql.DataFrame(jdf, sql_ctx)  [3.1]

     INFO Equivalent to Pandas and R DataFrame, BUT DISTRIBUTED


- Introduced in Spark 1.3 as extension to RDDs

- DISTRIBUTED collection of data organized into named columns
  
- Types of columns inferred from values

- Spark Python DataFrame performance = 5x RDD performance

- Easy to convert pySpark Dataframe <-> Pandas Dataframe
    
  WARNING pandas Dataframe must fit in Driver memory.#

  # Convert Spark DataFrame to Pandas 
  pandas_df = spark_df.toPandas()

  # Create a Spark DataFrame from Pandas
  spark_df = context.createDataFrame(pandas_df)


5.5) Semi-Structured Log Files

WARNING log files contain information supplied directly by the client, without
	escaping. Therefore, it is possible for malicious clients to insert
	control-characters in the log files, so care must be taken in dealing
	with raw logs.

5.5.1) Apache Web Server Log Format (GOTO Annex B)

• Apache Common Log Format specifies log file format -
  http://httpd.apache.org/docs/1.3/logs.html#common

• Example line from log file:

» 127.0.0.1 - - [01/Aug/1995:00:00:01 -0400] "GET /images/launch-logo.gif
  HTTP/1.0" 200 1839

• Components:
  » 127.0.0.1    => IP (or host name) of the requester (remote) client  .
  » -            => User id. from remote machine   ('-' == not available)
  » -            => user identity from local logon ('-' == not available)
  » [01/Aug/1995:00:00:01 -0400] => Request time (server finished the request)
  # 
  » "GET ... "   => First line of client request string:
  » 200          => Status code sent back to client: (2xx = OK); 3xx, 4xx, 5xx
  » 1839         => Size returned (0 if none)  

- Some log analysis questions: overall (sizes, statuses), temporal (xxx/per day)


5.6) Splunk: Data Mining ("Splunking")

- Collect & Analyze log files from many machines
- Check for unusual events
- Monitor resources
- Visualize with a dashboard.


5.7) File Performance - Summary

• Uncompressed read and write times are comparable
• Binary I/O is much faster than text I/O
• Compressed reads much faster than compressed writes
  » LZ4 is better than gzip
  » LZ4 compression times approach raw I/O times
• types of compression: lossy and lossless.


#===============================================================================
# 6) Lecture 6: Structured data - Relational Database
#===============================================================================

INTRO The Structure Spectrum

     • Structured data (schema-first)
     • Semi-Structured data (schema-later)
     • Unstructured (schema-never)

- data model: collection of CONCEPTS for describing data.

- schema: description of a particular collection of data, using a given data
  	  model.


6.1) Definitions

- Relational database: a set of relations

- Two parts to a Relation:

  Schema: (DB column-fields) specifies name of relation, plus each column’s
  	  name and type
	  
  Instance: the actual data at a given time
  	    • #rows = cardinality
  	    • #fields = degree

- Advantages
  » Well-defined structure
  » Maintains indices for high performance
  » Consistency maintained by transactions

- Disadvantages
  » Limited, rigid structure
  » Most of disk space is taken up by large indices
  » Transactions are slow
  » POOR SUPPORT FOR SPARSE DATA


6.2) The Structured Query Language (SQL)

- Simple query examples:

  • To find all 18 year old students:

    SELECT *
    FROM Students S
   WHERE S.age=18

   • To find just names and logins of all 18 year students:

  SELECT S.name, S.login
    FROM Students S
   WHERE S.age=18


- Equivalent SQL Join Notations

  • Explicit Join notation (preferred): (note.- default JOIN = INNER JOIN)

    SELECT S.name, E.classic
      FROM Students S INNER JOIN Enrolled E ON S.sid=E.sid


  • Implicit join notation (deprecated):

    SELECT S.name, E.classic
      FROM Students S Enrolled E
     WHERE S.sid=E.sid


- SQL Types of Joins

  • Inner Join (INNER JOIN): unmatched keys are ignored.

  • Left Join (LEFT OUTER JOIN): unmatched keys are used,
    	      	    	  	 but with right value = NULL

  • Right Join (RIGHT OUTER JOIN): unmatched keys are used,
    	      	    	  	   but with left value = NULL	


6.3) pyspark.sql and pySpark Joins

- pyspark.sql: Supported by pySpark DataFrames.

- Spark Joins

  • SparkSQL and Spark DataFrames join() supports:
    » inner, outer, left outer, right outer, semijoin

  • For Pair RDDs, pySpark supports:
    » inner join(), leftOuterJoin(), rightOuterJoin(), fullOuterJoin()


6.3.1) Pair RDD Joins

a) X.join(Y):  (k, (v1, v2)) tuple, where (k, v1) is in X and (k, v2) is in Y

   >>> x = sc.parallelize([("a",1),("b",4)])
   >>> y = sc.parallelize([("a",2),("a",3)])
   >>> sorted(x.join(y).collect())

   Value: [('a', (1, 2)), ('a', (1, 3))]


b) X.leftOuterJoin(Y) 

   >>> x = sc.parallelize([("a", 1), ("b", 4)])
   >>> y = sc.parallelize([("a", 2)])
   >>> sorted(x.leftOuterJoin(y).collect())

   Value: [('a', (1, 2)), ('b', (4, None))]

Hint: to calculate set_difference (X-Y), take only the pairs with 'None'.


c) Y.rightOuterJoin(X)

   >>> x = sc.parallelize([("a", 1), ("b", 4)])
   >>> y = sc.parallelize([("a", 2)])
   >>> sorted(y.rightOuterJoin(x).collect())

   Value: [('a', (2, 1)), ('b', (None, 4))]

Hint: to calculate set_difference (Y-X), take only the pairs with 'None'.


d) X.fullOuterJoin(Y)

   >>> x = sc.parallelize([("a", 1), ("b", 4)])
   >>> y = sc.parallelize([("a", 2), ("c", 8)])
   >>> sorted(x.fullOuterJoin(y).collect())

   Value: [('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]



#===============================================================================
# 7) Lecture 7: Data Quality
#===============================================================================

7.1) Data Cleaning

- Missing data, entity resolution, unit mismatch

- What’s the best trade-off between accuracy and simplicity?

7.2) Data Quality: Problems, Sources, and Continuum

- Problems
• (Source) Data is dirty on its own
• Transformations corrupt data (complexity of software pipelines)
• Clean datasets screwed up by integration (i.e., combining them)
• “Rare” errors can become frequent after transformation/integration
• Clean datasets can suffer “bit rot”: data loses value/accuracy over time
• Any combination of the above

- The Data Quality Continuum

  » Data gathering
  » Data delivery
  » Data storage
  » Data integration
  » Data retrieval
  » Data mining/analysis

7.3) Data Gathering, Delivery, Storage, Retrieval, Mining/Analysis

- Data Gathering – Potential Solutions

• Preemptive:
  » Process architecture (build in integrity checks)
  » Process management (reward accurate data entry, sharing, stewards)
• Retrospective:
  » Cleaning focus (duplicate removal, merge/purge, name/addr matching, field
  value standardization)
  » Diagnostic focus (automated detection of glitches)


- Data Delivery

Problems:

• Destroying/mutilating information by bad pre-processing
• Loss of data

Potential Solutions:

• Build reliable transmission protocols: use a relay server
• Verification: checksums, verification parser
• Interface agreements: Data quality commitment from data supplier


- Data Storage

Problems:

• Poor metadata
• Inappropriate data models
• Ad-hoc modifications.
• Hardware / software constraints.

Potential Solutions:

• Metadata: document and publish data specifications
• Planning
• Data exploration


- Data Retrieval

Problems:

• Source data not properly understood
• Computational constraints: Full history too expensive
• Incompatibility: ASCII? Unicode? UTF-8?

Potential Solutions:


- Data Mining and Analysis

Problems:

• What are you doing with all this data anyway?

• Problems in the analysis
  » Scale and performance
  » Confidence bounds?
  » Black boxes and dart boards
  » Attachment to models
  » Insufficient domain expertise
  » Casual empiricism (use arbitrary number to support a pre-conception)

• Poor metadata
• Inappropriate data models
• Ad-hoc modifications.
• Hardware / software constraints.

Potential Solutions:

• Data exploration
  » Determine which models and techniques are appropriate
  » Find data bugs
  » Develop domain expertise

• Continuous analysis
  » Are the results stable? How do they change?

• Accountability
  » Make the analysis part of the feedback loop


7.4) Data Quality Constraints and Metrics

• The constraints follow an 80-20 rule
  » A few constraints capture most cases,
  » Thousands of constraints to capture the last few cases

- Metrics

» Indicates what is wrong and how to improve
» Realize that DQ is a messy problem, no set of numbers will be perfect

- Examples of Data Quality Metrics
• Conformance to schema: evaluate constraints on a snapshot
• Conformance to business rules: evaluate constraints on DB changes
• Accuracy: perform expensive inventory or track complaints (proxy)
• Accessibility
• Interpretability
• Glitches in analysis
• Successful completion of end-to-end process

Technical Approaches
• Use multi-disciplinary approach to attack data quality problems
  » No one approach solves all problems
• Process Management: ensure proper procedures
• Statistics: focus on analysis – find and repair anomalies in data
• Database: focus on relationships – ensure consistency
• Metadata / Domain Expertise
  » What does data mean? How to interpret?

7.5) Data Integration

- Duplicate Record Detection (DeDup)

• Resolve multiple different entries:
  » Entity resolution, reference reconciliation, object ID/consolidation
• Remove Duplicates: Merge/Purge
• Record Linking (across data sources)
• Approximate Match (accept fuzziness)
• Householding (special case)
  » Different people in same house?

- Preprocessing/Standardization

Simple idea:
• Convert to canonical form (Example: mailing addresses)

More Sophisticated Techniques:
• Use evidence from multiple fields: Positive & Negative instances are possible
• Use evidence from linkage pattern with other records
• Clustering-based approaches

- Data Integration – Solutions

• Commercial Tools
  » Significant body of research in data integration
  » Many tools for address matching, schema mapping are available.
• Data browsing and exploration


#===============================================================================
# 8) Lecture 8: Exploratory Data Analysis and Machine Learning
#===============================================================================

8.1) Descriptive Statistics 

- Description our current data, not predicting the future.

- Two general types of statistic: Measures of central tendency and Measures of
  spread.


8.1.1) Exploratory Data Analysis [6]

- Set of techniques for VISUALIZING and SUMMARIZING data

- Goal: What can the data tell us? (vs “confirmatory” data analysis)

  • Important to look at data graphically before analyzing it

  • Basic statistics properties often fail to capture real-world complexities

- The five-number summary is a descriptive statistic that provides information
about a set of observations. It consists of the five most important sample
percentiles:

  • The sample minimum (smallest observation)
  • The lower quartile or first quartile
  • The median (middle value)
  • The upper quartile or third quartile
  • The sample maximum (largest observation)

  HINT You can compare the five-number summaries of multiple observations using
       a box plot.

- Central Limit Theorem: the distribution of sum (or mean) of n
  identically-distributed random variables Xi approaches a normal distribution
  as n → ∞


WARNING Understand your data’s distribution before applying any model


8.2) Inferential  Statistics

- Sampling: techniques that allow us to use small samples of the whole
population to make generalizations about the populations from which the samples
were drawn.


8.2.1) Machine learning techniques

a) Supervised Learning

        kNN (k Nearest Neighbors)

        Naive Bayes

        Logistic Regression

        Support Vector Machines

        Random Forests

b) Unsupervised Learning

        Clustering

        Factor Analysis

        Latent Dirichlet Allocation


8.2.2) Spark’s Machine Learning Toolkit

• mllib: scalable, distributed machine learning library
  » Scikit-learn like ML toolkit, Interoperates with NumPy

• Classification:
  » SVM, Logistic Regression, Decision Trees, Naive Bayes, …

• Regression: Linear, Lasso, Ridge, …

• Miscellaneous:
  » Alternating Least Squares, K-Means, SVD
  » Optimization primitives (SGD, L-BGFS)

Lab: Collaborative Filtering




################################################################################
Annex A) spark_tutorial_student
################################################################################

Web: http://localhost:8001/notebooks/spark_tutorial/spark_tutorial_student.ipynb

Local: ../labs/spark_tutorial_student.read-only.html 


- Result(read-only) Watch the saved web 

# Part 1-7) Context , transformations, actions

>>> type(sc)
>>> dir(sc)
>>> help(sc)
>>> sc.version

xrangeRDD = sc.parallelize(data, 8)

xrangeRDD.setName("Extra xrange RDD")

print "info: id ({0}), name ({1})".format(xrangeRDD.id(), xrangeRDD.name())

# View the lineage (set of transformations) of the RDD
print xrangeRDD.toDebugString()

xrangeRDD.getNumPartitions() # 8

# transformation
subRDD = xrangeRDD.map(sub)

# action: tasks will now be launched to perform the parallelize, map, and
# collect operations.
sub_rdd2 = xrangeRDD.map(lambda x: x-1)
collect_rdd2_list = sub_rdd2.collect()  # type list


# Part 8: Debugging Spark applications and lazy evaluation 


  INFO How Python is Executed in Spark

  Internally, Spark executes using a Java Virtual Machine (JVM). pySpark runs
  Python code in a JVM using Py4J. Py4J enables Python programs running in a
  Python interpreter to dynamically access Java objects in a Java Virtual
  Machine. Methods are called as if the Java objects resided in the Python
  interpreter and Java collections can be accessed through standard Python
  collection methods. Py4J also enables Java programs to call back Python
  objects.

  Because pySpark uses Py4J, coding errors often result in a complicated,
  confusing stack trace that can be difficult to understand. In the following
  section, we'll explore how to understand stack traces.


# (8c) Moving toward expert style

  (Recommended) Beginner code form:

  		RDD.transformation1()
		RDD.action1()
		RDD.transformation2()
		RDD.action2()

   Advanced code form:

   	    	RDD.transformation1().transformation2().action()


# (8d) Readability and code style: " ( one command per line ) "

       		(sc
		.parallelize(data)
		.map(lambda y: y - 1)
		.filter(lambda x: x < 10)
		.collect())


################################################################################
# References
################################################################################

[1] Spark Documentation - spark.apache.org/documentation.html

[2] Spark Programming Guide -spark.apache.org/docs/latest/programming-guide.html

[3] Spark Python API Docs - spark.apache.org/docs/latest/api/python/

    [3.1] pyspark DataFrame - https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame

[4] How-to: Tune Your Apache Spark Jobs - http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/

[5] Python plot lib (statistics graphs) - http://matplotlib.org/ 

[6] Exploratory Data Analysis - http://www.itl.nist.gov/div898/handbook/eda/eda_d.htm

