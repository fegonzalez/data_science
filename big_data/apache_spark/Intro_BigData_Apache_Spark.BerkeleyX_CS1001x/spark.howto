#\file spark.howto: Big Data - Data Science notes and facts.
       		    	       Apache Spark tutorial.




#===============================================================================
# INDEX
#===============================================================================

1&2) Lecture 1&2: Introduction.- What are Big Data and Data science

2) Lecture 3: Big Data, hardware, Map-Reduce vs Apache Spark

3) Lecture 3: Big Data & Cluster Computing: Map-Reduce vs Apache Spark

4) Lecture 4: Spark Essentials
   4.1)  RDDs
   4.2) Spark Program Lifecycle
   	4.2.1) Creating an RDD
	4.2.2) Spark Transformations
	4.2.3) Caching RDDs
	4.2.4) Spark Actions
   4.3) Spark Key-Value RDDs
   	4.3.1) Key-Value Transformations
   4.4) pySpark Closures
   4.5) pySpark Shared Variables
   	4.5.1) Broadcast Variables
   	4.5.2) Accumulators


Annex) To do section

References)


#===============================================================================
#===============================================================================


CONTENT


#===============================================================================
# 1&2) Lecture 1&2: Introduction.- What are Big Data and Data science
#===============================================================================

def) Big Data: data sets with sizes beyond the ability of commonly used
     software tools to properly handle it within a tolerable elapsed time =>
     data science to manage big data.

     - Big Data vs Data analytic: Volume, Velocity, Variety.


def) Data Science aims to derive knowledge from big data, efficiently and
     intelligently"


def) Data science encompasses the set of activities, tools, and methods that
     enable data-driven activities in science, business, medicine, ...

def) Data science = mix (hacking skills, domain expertise, math & stats know.)


1.1) Data Science vs Other systems/techniques

-  Data Science vs Databases

   Timing:       Querying the future  / Querying the past
   Data      :   massive & cheap      / modest volume & "precious" value
   Priorities:   Speed, availability  / consistency, error recovery, auditable
   Structured:   weak or none (test)  / strongly (schema)
   Realizations:      no SQL          / SQL 
   Properties:   CAP*, eventual cons. / Transactions, ACID+

   * CAP:  Consistency, Availability, Partition Tolerance
   + ACID: Atomicity, Consistency, Isolation, Durability


- Scientific Computing (Modeling) vs Data Science (Data-Driven Approach)

  Physics-based MODELS         / General INFERENCE ENGINE replaces model
  Problem-Structured           / Structure not related to problem
  Mostly DETERMINISTIC precise / Statistical models handle TRUE RANDOMNESS,
  	 	       	         and unmodeled complexity
  Run on SUPERCOMPUTER or      / Run on CHEAPER computer Clusters (EC2)
  Computing Cluster"


- Traditional Machine Learning vs Data Science

  Develop new (individual) models  / Explore many models, build and tune hybrids
  Prove math. properties of models / Understand empirical properties of models
  Improve/validate on a few,       / Develop/use tools that can handle MASSIVE
  relatively CLEAN, SMALL DATASETS   datasets
  Publish a paper                  / TAKE ACTION!


1.2) Data Science Topics

• Data Acquisition (where is the data?)
• Data Preparation (get the proper input data format)
• Analysis
• Data Presentation
• Data Products
• Observation and Experimentation


1.3) Data Acquisition & Data Preparation

1.3.1 Overview

• ETL: Extract, Transform, Load
  » We need to extract data from the source(s)
  » We need to load data into the sink
  » We need to transform data at the source, sink, or in a staging area

- Sources: files (csv, excel, logs), databases, web site, ...
- Sinks: code files (Python, R, ...), DB manage systems, ...

1.3.2) Process Model

• The construction of a new data preparation process is done in many phases:
  » Data characterization
  » Data cleaning
  » Data integration

• We must efficiently move data around in space and time
  » Data transfer
  » Data serialization and deserialization (for files or network)

WARNING Impediments to Collaboration:
  » DIVERSITY of tools (sources & sinks)
  » No/bad documentation and version controlling.




#===============================================================================
# 3) Lecture 3: Big Data & Cluster Computing: Map-Reduce vs Apache Spark
#===============================================================================

- The Big Data Problem
  • A single machine can no longer process or even store all the data!
  • Only solution is to DISTRIBUTE DATA over large clusters


3.1) Map Reduce: disk => MAP => disk => REDUCE => disk

     PROBLEM: disk I/O is very slow!


3.2) Spark (Distributed execution engine)

    SOLUTION: In-Memory Data sharing (use memory instead of disk)


3.2.1) Spark Core and Resilient Distributed Datasets (RDDs)

- Spark Core: provides distributed task dispatching, scheduling, and basic I/O.

  • Provides PROGRAMMING ABSTRACTION and PARALLEL RUNTIME to hide complexities
    of fault-tolerance and slow machines.

  • “Here’s an operation, run it on all of the data”
    » I don’t care where it runs (you schedule that)
    » In fact, feel free to run it twice on different nodes


- Resilient (*1) Distributed Datasets (RDDs):

  • It is the fundamental programming abstraction, A LOGICAL COLLECTION OF DATA
    PARTITIONED ACROSS MACHINES, stored in memory or in disk.
  
  • RDDs can be created by referencing datasets in external storage systems, or
    by applying a diverse set of parallel transformations (map, filter, join)
    and actions (count, collect, save).

  • The RDD abstraction is exposed through a language-integrated API ->
    -> manipulate RDDs is similar to manipulating local collections of data.

  • RDDs automatically rebuilt on machine failure"


  (*1) resilient = elástico


- Spark Tools
  • Spark SQL
  • Spark Streaming
  • MLlib (machine learning)
  • GraphX (graph)
  

- Spark and Map Reduce Differences 

  	    	Hadoop			Spark
	    	Map Reduce
________________________________________________________________________
             |                    |
Storage      |	Disk only	  |	In-memory or on disk
             |                    |
Operations   |	Map and Reduce	  |	Map, Reduce, Join, Sample, etc…
             |                    |
Execution    |	Batch   	  |	Batch, interactive, streaming
model	     |                    |
             |                    |
Programming  |	Java		  |	Scala, Java, R, and Python
environments |                    |


- Other Spark advantages

  • Generalized patterns -> unified engine for many use cases

  • Lazy evaluation of the lineage graph -> reduces wait states, best pipelining

  • Lower overhead for starting jobs

  • Less expensive shuffles




#===============================================================================
# 4) Lecture 4: Spark Essentials
#===============================================================================

- pySpark: python spark API

- Spark program = 2 programs = 1 driver program + n worker programs


- Spark driver program (Spark Context Object)

  • Runs the user’s main function and executes various parallel operations

  • Runs on a (UNIQUE) CLUSTER.


- Worker programs (Spark executor Object): 

  • Run in cluster nodes or in local threads.

  • RDDs are distributed across workers


- class pyspark.SparkContext()

  • A spark program first creates a "SparkContext object"

  • Represents the connection to a Spark cluster: tells Spark how and where to
    access a cluster.

  • Can be used to create RDDs and broadcast variables on that cluster.

  • The "Master" parameter determines which type and size of cluster to use:
    local, local[k], spark cluster, mesos cluster.


INFO pySpark shell and Databricks Cloud automatically create the "sc" variable

INFO iPython and programs must use a constructor to create a new SparkContext


4.1)  RDDs

  A logical collection of data partitioned across machines.

  » Immutable once constructed

  » Track lineage information to efficiently recompute lost data

  » Enable OPERATIONS on collection of elements IN PARALLEL

- How to create an RDD:
  i)   by PARALLELIZING existing Python collections (lists)
  ii)  by TRANSFORMING an existing RDDs
  iii) FROM FILES in HDFS or any other storage system

- The programmer specifies number of partitions for an RDD: more partitions =
  more parallelism.

  WARNING  #partitions !=  #workers  (e.g. 5 partitions can run in 3 workers)


4.2) Spark Program Lifecycle

  i)   Create an RDD
  ii)  Apply transformations to an RDD (lazy eval.)
  iii) cache() some RDDs for reuse
  iv)  Apply actions to an RDD


WARNING What code is executed in what program. Summary:

 • RDDs creation are executed (in parallel) in the workers.

 • Transformations are executed (in parallel) in the workers.

 • Actions are executed (in parallel) in the workers

 • Actions result-combination is executed in the driver.


4.2.1) Creating an RDD

WARNING lazy evaluation -> Spark only records what to create.
 
a) From Python collections(lists)

  parallelize(collection, numSlices=None) [3].pyspark

INFO Using xrange is recommended if the input represents a range for performance

>>> data = [1,2,3,4,5]
>>> rdd = sc.parallelize(data, 4) # RDD distributed in 4 partitions


b) From files (text, HDFS, ...)

  textFile(name, minPartitions=None, use_unicode=True) [3].pyspark

• Read a text file from HDFS, a local file system (available on all nodes), or
  any Hadoop-supported file system URI, and return it as an RDD of Strings.

• Elements are lines of input.

>>> distFile = sc.textFile("README.md", 4) 


4.2.2) Spark Transformations

• Create new datasets from an existing one

• Transformations specify how to perform parallel computation in a LAZILY
  EVALUATED manner.

INFO Spark optimizes the required calculations

INFO Spark recovers from failures and slow workers


- map(func): func(input) | dataset

  >>> rdd = sc.parallelize([1, 2, 3, 4])
  >>> rdd.map(lambda x: x * 2)
  RDD: [1, 2, 3, 4] -> [2, 4, 6, 8]


- filter(func):  func(input)==true | dataset

  >>> rdd = sc.parallelize([1, 2, 3, 4])
  >>> rdd.filter(lambda x: x % 2 == 0)
  RDD: [1, 2, 3, 4] -> [2, 4]


- distinct([numTasks]): distinct(input) | dataset

  >>> rdd = sc.parallelize([1, 4, 2, 2, 3, 4])
  >>> rdd.distinct()
  RDD: [1, 4, 2, 2, 3, 4] -> [1, 4, 2, 3]


- flatMap(func): func(input) | dataset

  >>> rdd = sc.parallelize([1, 2, 3])

  >>> rdd.map(lambda x: [x, x+5])
  RDD: [1, 2, 3] -> [[1,6], [2,7], [3,8]]

  >>> rdd.flatmap(lambda x: [x, x+5])
  RDD: [1, 2, 3] -> [1, 6, 2, 7, 3, 8]


WARNING Function literals (lambda) are closures. Spark automatically pushes
	closures to workers.
  

4.2.3) Caching RDDs: 

- cache(): Persist this RDD with the default storage level (MEMORY_ONLY_SER).

  >>> lines = sc.textfile("...", 4)
  >>> lines.cache() # save, don't recompute inside the print()
  >>> comments = lines.filter(iscomment)
  >>> print (lines.count(), comments.count())


4.2.4) Spark Actions

• Actions cause the transformations to be executed.  

• Mechanism for getting results out of Spark.


- reduce(func)

  • aggregate dataset’s elements using function func.

  • func takes two arguments and returns one.

  • func is commutative and associative so that it can be computed correctly in
    parallel.

  >>> rdd = sc.parallelize([1, 2, 3])
  >>> rdd.reduce(lambda a,b: a * b)    # causes parallelize to be executed
  Value: 6  # (1*2*3)


- take(n): return an array with the first n elements of the RDD

  >>> rdd.take(2)
  Value: [1, 2]


- collect(): return a list that contains ALL of the elements in this RDD.

  >>> rdd.collect()
  Value: [1,2,3]

  WARNING make sure will fit in driver program
  • Forum note.- https://piazza.com/class/i9esrcg0gpf8k?cid=799
  • Forum note (Anthony D. Joseph).- You can see the sizes of RDDs and worker
    memory in the Spark UI under the Executors tab.


- count(): Return the number of elements in this RDD

  >>> rdd.count()
  Value: 3


- takeOrdered(n, key=None): return n elements ordered in ascending order or as
  		 	    specified by the optional key function.

  >>> rdd = sc.parallelize([5, 3, 1, 2])
  >>> rdd.takeOrdered(3)
  Value: [1,2,3]  
  >>> rdd.takeOrdered(3, lambda s: -1 * s)
  Value: [5,3,2]


4.3) Spark Key-Value RDDs

• Similar to Map Reduce, Spark supports Key-Value pairs

• Each element of a Pair RDD is a pair tuple

  >>> rdd = sc.parallelize([(1,2),(3,4)])
  RDD:[(1,2),(3,4)]


4.3.1) Key-Value Transformations

- reduceByKey(func, numPartitions=None): func(input) | dataset of (K,V) pairs
  • func must be of type (V,V) -> V
  • Output will be hash-partitioned with numPartitions partitions

  >>> rdd = sc.parallelize([(1, 2), (3, 4), (3,6)])
  >>> rdd.map(lambda a,b: a+b)
  RDD: [(1, 2), (3, 4), (3,6)] -> [(1, 2), (3, 10)]
  

- sortByKey(): return a new dataset (K, V) pairs sorted by keys in ascending
  	       order.

  >>> rdd = sc.parallelize([(1, 'a'), (2, 'c'), (1, 'b')])
  >>> rdd.sortByKey()
  RDD: [(1, 'a'), (2, 'c'), (1, 'b')] -> [(1, 'a'), (1, 'b'), (2, 'c')]
  

- groupByKey(): return a new dataset of (K, iterable<V>) pairs.

  >>> rdd = sc.parallelize([(1, 'a'), (2, 'c'), (1, 'b')])
  >>> rdd.groupByKey()
  RDD: [(1, 'a'), (2, 'c'), (1, 'b')] -> [(1, ['a', 'b']), (2, ['c'])]

  WARNING groupByKey() as it can cause a lot of data movement across the
  	  network and create large Iterables at workers.


4.4) pySpark Closures  (driver -> worker)

• Spark automatically creates closures for:
  » Functions that run on RDDs at workers
  » Any global variables used by those workers

• One closure per worker
  » Sent for every task
  » No communication between workers
  » Changes to global variables at workers are not sent to driver

Problems:
  » Closures are (re-)sent with every job
  » Inefficient to send large data to each worker
  » Closures are one way: driver -> worker

Solution: pySpark Shared Variables


4.5) pySpark Shared Variables

4.5.1) Broadcast Variables (driver -> worker)

  » SHIP to each worker ONLY ONCE instead of with each task
  » On workers: read-only, cached.

  - Creation (driver)   variable = sc.broadcast()
  - Access (workers)    variable.value

  >>> broadvar = sc.broadcast([1,2,3])  # driver
  >>> broadvar.value  # At the worker (in code passed via closure)
  [1, 2, 3]


4.5.2) Accumulators  (worker -> driver)

       - accumulator(value, accum_param=None): create & init an accumulator

       - value: get the accumulator value (driver)

       - add(term): commutative and associative “add” operation (worker)


• Tasks at workers cannot access accumulator’s values (write-only)

• Used to: Aggregate values from workers back to driver (e.g. PARALLEL counters
       	   and sums)

• Accumulators can be used in actions or transformations:  
  » Actions: each task’s update to accumulator is applied only once
  WARNING Transformations: no guarantees (use only for debugging)

• Types
  » SparkContext: integers, double, long, float
  » User defined: via AccumulatorParam object (See lab for example of custom
    type)


• (e.g. counting empty lines)

  file = sc.textFile(input_file)   # Create RDD
  blank_lines = sc.accumulator(0)  # Create & init Accumulator[Int]
  
  # def a function that use the accumulator inside
  def extract_call_signs(line)
      global blank_lines  # make accessible the global accumulator variable 
      if (line==""):
      	 blank_lines+=1   # or 'add(blank_lines)'
      return line.split(" ")

  call_signs = file.flatMap(extract_call_signs)
  print ("blank lines: %d" % blank_lines.value)  # (driver) get value



################################################################################
# Annex) To do section
################################################################################




################################################################################
# References
################################################################################

[1] Spark Documentation - spark.apache.org/documentation.html

[2] Spark Programming Guide -spark.apache.org/docs/latest/programming-guide.html

[3] Spark Python API Docs - spark.apache.org/docs/latest/api/python/

[4] How-to: Tune Your Apache Spark Jobs - http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/

