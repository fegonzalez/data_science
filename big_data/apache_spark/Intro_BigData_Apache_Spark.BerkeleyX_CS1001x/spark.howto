#\file spark.howto: Big Data - Data Science notes and facts.
       		    	       Apache Spark tutorial.




#===============================================================================
# INDEX
#===============================================================================

1&2) Lecture 1&2: Introduction.- What are Big Data and Data science

2) Lecture 3: Big Data, hardware, Map-Reduce vs Apache Spark

3) Lecture 3: Big Data & Cluster Computing: Map-Reduce vs Apache Spark

4) Lecture 4: Spark Essentials
   4.1) RDDs
   4.2) Spark Program Lifecycle
   	4.2.1) Creating an RDD
	4.2.2) Spark Transformations
	4.2.3) Caching RDDs and storage options
	       4.2.3.1) Caching RDDs
	       4.2.3.2) Unpersist and storage options
	4.2.4) Spark Actions
   4.3) Spark Key-Value RDDs
   	4.3.1) Key-Value Transformations
   4.4) pySpark Closures
   4.5) pySpark Shared Variables
   	4.5.1) Broadcast Variables
   	4.5.2) Accumulators


Annex) spark_tutorial_student

Annex) To do section

References)


#===============================================================================
#===============================================================================


CONTENT


#===============================================================================
# 1&2) Lecture 1&2: Introduction.- What are Big Data and Data science
#===============================================================================

def) Big Data: data sets with sizes beyond the ability of commonly used
     software tools to properly handle it within a tolerable elapsed time =>
     data science to manage big data.

     - Big Data vs Data analytic: Volume, Velocity, Variety.


def) Data Science aims to derive knowledge from big data, efficiently and
     intelligently"


def) Data science encompasses the set of activities, tools, and methods that
     enable data-driven activities in science, business, medicine, ...

def) Data science = mix (hacking skills, domain expertise, math & stats know.)


1.1) Data Science vs Other systems/techniques

-  Data Science vs Databases

   Timing:       Querying the future  / Querying the past
   Data      :   massive & cheap      / modest volume & "precious" value
   Priorities:   Speed, availability  / consistency, error recovery, auditable
   Structured:   weak or none (test)  / strongly (schema)
   Realizations:      no SQL          / SQL 
   Properties:   CAP*, eventual cons. / Transactions, ACID+

   * CAP:  Consistency, Availability, Partition Tolerance
   + ACID: Atomicity, Consistency, Isolation, Durability


- Scientific Computing (Modeling) vs Data Science (Data-Driven Approach)

  Physics-based MODELS         / General INFERENCE ENGINE replaces model
  Problem-Structured           / Structure not related to problem
  Mostly DETERMINISTIC precise / Statistical models handle TRUE RANDOMNESS,
  	 	       	         and unmodeled complexity
  Run on SUPERCOMPUTER or      / Run on CHEAPER computer Clusters (EC2)
  Computing Cluster"


- Traditional Machine Learning vs Data Science

  Develop new (individual) models  / Explore many models, build and tune hybrids
  Prove math. properties of models / Understand empirical properties of models
  Improve/validate on a few,       / Develop/use tools that can handle MASSIVE
  relatively CLEAN, SMALL DATASETS   datasets
  Publish a paper                  / TAKE ACTION!


1.2) Data Science Topics

• Data Acquisition (where is the data?)
• Data Preparation (get the proper input data format)
• Analysis
• Data Presentation
• Data Products
• Observation and Experimentation


1.3) Data Acquisition & Data Preparation

1.3.1 Overview

• ETL: Extract, Transform, Load
  » We need to extract data from the source(s)
  » We need to load data into the sink
  » We need to transform data at the source, sink, or in a staging area

- Sources: files (csv, excel, logs), databases, web site, ...
- Sinks: code files (Python, R, ...), DB manage systems, ...

1.3.2) Process Model

• The construction of a new data preparation process is done in many phases:
  » Data characterization
  » Data cleaning
  » Data integration

• We must efficiently move data around in space and time
  » Data transfer
  » Data serialization and deserialization (for files or network)

WARNING Impediments to Collaboration:
  » DIVERSITY of tools (sources & sinks)
  » No/bad documentation and version controlling.




#===============================================================================
# 3) Lecture 3: Big Data & Cluster Computing: Map-Reduce vs Apache Spark
#===============================================================================

- The Big Data Problem
  • A single machine can no longer process or even store all the data!
  • Only solution is to DISTRIBUTE DATA over large clusters


3.1) Map Reduce: disk => MAP => disk => REDUCE => disk

     PROBLEM: disk I/O is very slow!


3.2) Spark (Distributed execution engine)

    SOLUTION: In-Memory Data sharing (use memory instead of disk)


3.2.1) Spark Core and Resilient Distributed Datasets (RDDs)

- Spark Core: provides distributed task dispatching, scheduling, and basic I/O.

  • Provides PROGRAMMING ABSTRACTION and PARALLEL RUNTIME to hide complexities
    of fault-tolerance and slow machines.

  • “Here’s an operation, run it on all of the data”
    » I don’t care where it runs (you schedule that)
    » In fact, feel free to run it twice on different nodes


- Resilient (*1) Distributed Datasets (RDDs):

  • It is the fundamental programming abstraction, A LOGICAL COLLECTION OF DATA
    PARTITIONED ACROSS MACHINES, stored in memory or in disk.
  
  • RDDs can be created by referencing datasets in external storage systems, or
    by applying a diverse set of parallel transformations (map, filter, join)
    and actions (count, collect, save).

  • The RDD abstraction is exposed through a language-integrated API ->
    -> manipulate RDDs is similar to manipulating local collections of data.

  • RDDs automatically rebuilt on machine failure"


  (*1) resilient = elástico


- Spark Tools
  • Spark SQL
  • Spark Streaming
  • MLlib (machine learning)
  • GraphX (graph)
  

- Spark and Map Reduce Differences 

  	    	Hadoop			Spark
	    	Map Reduce
________________________________________________________________________
             |                    |
Storage      |	Disk only	  |	In-memory or on disk
             |                    |
Operations   |	Map and Reduce	  |	Map, Reduce, Join, Sample, etc…
             |                    |
Execution    |	Batch   	  |	Batch, interactive, streaming
model	     |                    |
             |                    |
Programming  |	Java		  |	Scala, Java, R, and Python
environments |                    |


- Other Spark advantages

  • Generalized patterns -> unified engine for many use cases

  • Lazy evaluation of the lineage graph -> reduces wait states, best pipelining

  • Lower overhead for starting jobs

  • Less expensive shuffles




#===============================================================================
# 4) Lecture 4: Spark Essentials
#===============================================================================

- pySpark: python spark API


- Spark program = 2 programs = 1 driver program + n worker programs

  The driver has Spark jobs that it needs to run and these jobs are split into
  tasks that are submitted to the executors for completion. The results from
  these tasks are delivered back to the driver.

  i) Assigning a job (driver)

     job (stage): a stage is a group of tasks that all perform the same
     	 	  computation, but on different input data

     1 job (stage) -> n tasks
     1 task -> executed in 1 partition within an executor (worker)

  ii) Resolving tasks in parallel (workers)

  iii) Resolving the job

      n tasks results delivered back to the driver
      driver combine the results

      WARNING the data returned to the driver must fit into the driver's
      	      available memory. If not, the driver will crash.


- Spark driver program (SparkContext Object)

  • Runs the user’s main function and executes various parallel operations

  • Runs on a (UNIQUE) CLUSTER.


- Worker programs (executor Object): 

  • Run in cluster nodes or in local threads.

  • RDDs are distributed across workers


- class pyspark.SparkContext()

  • A spark program first creates a "SparkContext object"

  • Represents the connection to a Spark cluster: tells Spark how and where to
    access a cluster.

  • Can be used to create RDDs and broadcast variables on that cluster.

  • The "Master" parameter determines which type and size of cluster to use:
    local, local[k], spark cluster, mesos cluster.


INFO pySpark shell and Databricks Cloud automatically create the "sc" variable

INFO iPython and programs must use a constructor to create a new SparkContext


4.1) RDDS  [3]#pyspark.RDD

     RDD = logical collection of data partitioned across machines:

    - In Spark, datasets are represented as a list of entries

    - the list is broken up into many different partitions-

    - Each partition stored on a different machine.

    - Each partition holds a UNIQUE subset of the entries in the list.

    - Partitions are each stored in a worker's memory


  » Immutable once constructed

  » Track lineage information to efficiently recompute lost data

  » Enable OPERATIONS on collection of elements IN PARALLEL

- How to create an RDD:
  i)   by PARALLELIZING existing Python collections (lists)
  ii)  by TRANSFORMING an existing RDDs
  iii) FROM FILES in HDFS or any other storage system

- The programmer specifies number of partitions for an RDD: more partitions =
  more parallelism.

  WARNING  #partitions !=  #workers  (e.g. 5 partitions can run in 3 workers)


4.2) Spark Program Lifecycle

  i)   Create an RDD
  ii)  Apply transformations to an RDD (lazy eval.)
  iii) cache() some RDDs for reuse
  iv)  Apply actions to an RDD


WARNING What code is executed in what program. Summary:

 • RDDs creation are executed (in parallel) in the workers.

 • Transformations are executed (in parallel) in the workers.

 • Actions are executed (in parallel) in the workers

 • Actions result-combination is executed in the driver.


4.2.1) Creating an RDD

WARNING lazy evaluation -> Spark only records what to create.
 
a) From Python collections(lists)

  parallelize(collection, numSlices=None) [3].pyspark

INFO Using xrange is recommended if the input represents a range for performance

>>> data = [1,2,3,4,5]
>>> rdd = sc.parallelize(xra, 4) # RDD distributed in 4 partitions

>>> rdd.id() # each RDD gets a unique ID [1, 2, ...]


b) From files (text, HDFS, ...)

  textFile(name, minPartitions=None, use_unicode=True) [3].pyspark

• Read a text file from HDFS, a local file system (available on all nodes), or
  any Hadoop-supported file system URI, and return it as an RDD of Strings.

• Elements are lines of input.

>>> distFile = sc.textFile("README.md", 4) 


4.2.2) Spark Transformations

WARNING RDDs inmutables => each transformation actually creates a new RDD.

• Create new datasets from an existing one

• Transformations specify how to perform parallel computation in a LAZILY
  EVALUATED manner.

INFO Spark optimizes the required calculations

INFO Spark recovers from failures and slow workers


WARNING Function literals (lambda) are closures. Spark automatically pushes
	closures to workers.

INFO You can see the set of transformations that were applied to create an RDD
     by using the toDebugString() method: print myrdd.toDebugString()
  

- map(func): func(input) | dataset

  >>> rdd = sc.parallelize([1, 2, 3, 4])
  >>> rdd.map(lambda x: x * 2)
  RDD: [1, 2, 3, 4] -> [2, 4, 6, 8]


- filter(func):  func(input)==true | dataset

  >>> rdd = sc.parallelize([1, 2, 3, 4])
  >>> rdd.filter(lambda x: x % 2 == 0)
  RDD: [1, 2, 3, 4] -> [2, 4]


- distinct([numTasks]): distinct(input) | dataset

  >>> rdd = sc.parallelize([1, 4, 2, 2, 3, 4])
  >>> rdd.distinct()
  RDD: [1, 4, 2, 2, 3, 4] -> [1, 4, 2, 3]


- flatMap(func): func(input) | dataset

  >>> rdd = sc.parallelize([1, 2, 3, 2])

  >>> rdd.map(lambda x: [x, x+5])
  RDD: [1, 2, 3, 2] -> [[1,6], [2,7], [3,8], [2,7]]

  >>> rdd.flatmap(lambda x: [x, x+5])
  RDD: [1, 2, 3, 2] -> [1, 6, 2, 7, 3, 8, 2, 7]


- mapPartitions(f, preservesPartitioning=False): 

  Return a new RDD by applying a function to each partition of this RDD.

    >>> rdd = sc.parallelize([1, 2, 3, 4], 2)
    >>> def f(iterator): yield sum(iterator)
    >>> rdd.mapPartitions(f).collect()
    [3, 7] # partition 1, sum[1,2]; partition 2, sum [3,4] 


- mapPartitionsWithIndex(f, preservesPartitioning=False): 

  Return a new RDD by applying a function to each partition of this RDD, while
  tracking the index of the original partition. For every partition (index,
  iterator) pair, the function returns a tuple of the same partition index
  number and an iterator of the transformed items in that partition.

    >>> rdd = sc.parallelize([1, 2, 3, 4], 4)
    >>> def f(splitIndex, iterator): yield splitIndex
    >>> rdd.mapPartitionsWithIndex(f).sum()
    6


4.2.3) Caching RDDs and storage options 

4.2.3.1) Caching RDDs

INFO For efficiency Spark keeps your RDDs in memory. By keeping the contents in
     memory, Spark can quickly access the data. However, memory is limited, so
     if you try to keep too many RDDs in memory, Spark will automatically
     delete RDDs from memory to make space for new RDDs. If you later refer to
     one of the RDDs, Spark will automatically recreate the RDD for you, but
     that takes time.  So, if you plan to use an RDD more than once, then you
     should tell Spark to cache that RDD.  

     However, if you cache too many RDDs and Spark runs out of memory, it will
     delete the least recently used (LRU) RDD first. Again, the RDD will be
     automatically recreated when accessed.


- cache(): Persist this RDD with the default storage level (MEMORY_ONLY_SER).

  >>> lines = sc.textfile("...", 4)
  >>> lines.cache() # save, don't recompute inside the print()
  >>> comments = lines.filter(iscomment)
  >>> print (lines.count(), comments.count())

- is_cached(): check if an RDD is cached 

  >>> print(lines.is_cached()
  True


4.2.3.2) Unpersist and storage options

INFO Spark automatically manages the RDDs cached in memory and will save them
     to disk if it runs out of memory. For efficiency, once you are finished
     using an RDD, you can optionally tell Spark to stop caching it in memory
     by using the RDD's unpersist() method to inform Spark that you no longer
     need the RDD in memory.

- unpersist(): Mark the RDD as non-persistent, and remove all blocks for it
  	       from memory AND DISK.


INFO you can directly query the current storage information for an RDD using
     the getStorageLevel() operation.

  filteredRDD.unpersist()
  print "Storage level for a non cached RDD: ", filteredRDD.getStorageLevel()
  filteredRDD.cache()
  print "Storage level for a cached RDD:     ", filteredRDD.getStorageLevel()
  #
  # Result:
  # Storage level for a non cached RDD:  Serialized 1x Replicated
  # Storage level for a cached RDD:      MEMORY Serialized 1x Replicated


- persist(storageLevel=StorageLevel(False, True, False, False, 1))

  Set this RDD’s storage level to persist its values across operations after
  the first time it is computed.


4.2.4) Spark Actions

• Actions cause the transformations to be executed.  

• Mechanism for getting results out of Spark.


- reduce(func)

  • aggregate dataset’s elements using function func.

  • func takes two arguments and returns one.

  • func MUST BE commutative and associative so that it can be computed
    correctly in paralle. Otherwise, the results from reduce() will be
    inconsistent.

  >>> rdd = sc.parallelize([1, 2, 3])
  >>> rdd.reduce(lambda a,b: a * b)    # causes parallelize to be executed
  Value: 6  # (1*2*3)


- take(n): return an array with the first n elements of the RDD

  >>> rdd.take(2)
  Value: [1, 2]


- first(): return firsst element == take(1)


- collect(): return a list that contains ALL of the elements in this RDD.

  >>> rdd.collect()
  Value: [1,2,3]

  WARNING make sure will fit in driver program
  • Forum note.- https://piazza.com/class/i9esrcg0gpf8k?cid=799
  • Forum note (Anthony D. Joseph).- You can see the sizes of RDDs and worker
    memory in the Spark UI under the Executors tab.


- count(): Return the number of elements in this RDD

  >>> rdd.count()
  Value: 3


- takeOrdered(n, key=None): return n elements ordered in ascending order or as
  		 	    specified by the optional key function.

  >>> rdd = sc.parallelize([5, 3, 1, 2])
  >>> rdd.takeOrdered(3)
  Value: [1,2,3]  
  >>> rdd.takeOrdered(3, lambda s: -1 * s)
  Value: [5,3,2]

- top(num, key=None): Get the top N elements from a RDD.

  Note: It returns the list sorted in descending order. (inverse to takeOrdered)

  >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)
  [12]
  >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)
  [6, 5]
  >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)
  [4, 3, 2]


- takeSample(withReplacement, num, seed=None): return a fixed-size sampled
   subset of this RDD.


- countByValue(): Return the count of each unique value in this RDD as a
  dictionary of (value, count) pairs.

  >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())
  [(1, 2), (2, 3)] # value 1: twice; value 2: three times


4.3) Spark Key-Value RDDs

• Similar to Map Reduce, Spark supports Key-Value pairs

• Each element of a Pair RDD is a pair tuple

  >>> rdd = sc.parallelize([(1,2),(3,4)])
  RDD:[(1,2),(3,4)]


4.3.1) Key-Value Transformations

- reduceByKey(func, numPartitions=None): func(input) | dataset of (K,V) pairs
  • func must be of type (V,V) -> V
  • Output will be hash-partitioned with numPartitions partitions

  >>> rdd = sc.parallelize([(1, 2), (3, 4), (3,6)])
  >>> rdd.map(lambda a,b: a+b)
  RDD: [(1, 2), (3, 4), (3,6)] -> [(1, 2), (3, 10)]
  

- sortByKey(): return a new dataset (K, V) pairs sorted by keys in ascending
  	       order.

  >>> rdd = sc.parallelize([(1, 'a'), (2, 'c'), (1, 'b')])
  >>> rdd.sortByKey()
  RDD: [(1, 'a'), (2, 'c'), (1, 'b')] -> [(1, 'a'), (1, 'b'), (2, 'c')]
  

- groupByKey(): return a new dataset of (K, iterable<V>) pairs.

  >>> rdd = sc.parallelize([(1, 'a'), (2, 'c'), (1, 'b')])
  >>> rdd.groupByKey()
  RDD: [(1, 'a'), (2, 'c'), (1, 'b')] -> [(1, ['a', 'b']), (2, ['c'])]


WARNING groupByKey() can cause a lot of data movement across the network and
  	create large Iterables at workers.

WARNING While both the groupByKey() and reduceByKey() transformations can often
	be used to solve the same problem and will produce the same answer, THE
	reducebykey() TRANSFORMATION WORKS MUCH BETTER FOR LARGE DISTRIBUTED
	DATASETS. This is BECAUSE Spark knows it can COMBINE OUTPUT with a
	common key on each partition before shuffling (redistributing) the data
	across nodes. Only use groupByKey() if the operation would not benefit
	from reducing the data before the shuffle occurs.



INFO	Here are more transformations to prefer over groupByKey():

- combineByKey() can be used when you are combining elements but your return
  type differs from your input value type.

- foldByKey() merges the values for each key using an associative function and
  a neutral "zero value".


4.4) pySpark Closures  (driver -> worker)

• Spark automatically creates closures for:
  » Functions that run on RDDs at workers
  » Any global variables used by those workers

• One closure per worker
  » Sent for every task
  » No communication between workers
  » Changes to global variables at workers are not sent to driver

Problems:
  » Closures are (re-)sent with every job
  » Inefficient to send large data to each worker
  » Closures are one way: driver -> worker

Solution: pySpark Shared Variables


4.5) pySpark Shared Variables

4.5.1) Broadcast Variables (driver -> worker)

  » SHIP to each worker ONLY ONCE instead of with each task
  » On workers: read-only, cached.

  - Creation (driver)   variable = sc.broadcast()
  - Access (workers)    variable.value

  >>> broadvar = sc.broadcast([1,2,3])  # driver
  >>> broadvar.value  # At the worker (in code passed via closure)
  [1, 2, 3]


4.5.2) Accumulators  (worker -> driver)

       - accumulator(value, accum_param=None): create & init an accumulator

       - value: get the accumulator value (driver)

       - add(term): commutative and associative “add” operation (worker)


• Tasks at workers cannot access accumulator’s values (write-only)

• Used to: Aggregate values from workers back to driver (e.g. PARALLEL counters
       	   and sums)

• Accumulators can be used in actions or transformations:  
  » Actions: each task’s update to accumulator is applied only once
  WARNING Transformations: no guarantees (use only for debugging)

• Types
  » SparkContext: integers, double, long, float
  » User defined: via AccumulatorParam object (See lab for example of custom
    type)


• (e.g. counting empty lines)

  file = sc.textFile(input_file)   # Create RDD
  blank_lines = sc.accumulator(0)  # Create & init Accumulator[Int]
  
  # def a function that use the accumulator inside
  def extract_call_signs(line)
      global blank_lines  # make accessible the global accumulator variable 
      if (line==""):
      	 blank_lines+=1   # or 'add(blank_lines)'
      return line.split(" ")

  call_signs = file.flatMap(extract_call_signs)
  print ("blank lines: %d" % blank_lines.value)  # (driver) get value



################################################################################
Annex) spark_tutorial_student
################################################################################

Web: http://localhost:8001/notebooks/spark_tutorial/spark_tutorial_student.ipynb

Local: ../labs/spark_tutorial_student.read-only.html 


- Result(read-only) Watch the saved web 

# Part 1-7) Context , transformations, actions

>>> type(sc)
>>> dir(sc)
>>> help(sc)
>>> sc.version

xrangeRDD = sc.parallelize(data, 8)

xrangeRDD.setName("Extra xrange RDD")

print "info: id ({0}), name ({1})".format(xrangeRDD.id(), xrangeRDD.name())

# View the lineage (set of transformations) of the RDD
print xrangeRDD.toDebugString()

xrangeRDD.getNumPartitions() # 8

# transformation
subRDD = xrangeRDD.map(sub)

# action: tasks will now be launched to perform the parallelize, map, and
# collect operations.
sub_rdd2 = xrangeRDD.map(lambda x: x-1)
collect_rdd2_list = sub_rdd2.collect()  # type list


# Part 8: Debugging Spark applications and lazy evaluation 


  INFO How Python is Executed in Spark

  Internally, Spark executes using a Java Virtual Machine (JVM). pySpark runs
  Python code in a JVM using Py4J. Py4J enables Python programs running in a
  Python interpreter to dynamically access Java objects in a Java Virtual
  Machine. Methods are called as if the Java objects resided in the Python
  interpreter and Java collections can be accessed through standard Python
  collection methods. Py4J also enables Java programs to call back Python
  objects.

  Because pySpark uses Py4J, coding errors often result in a complicated,
  confusing stack trace that can be difficult to understand. In the following
  section, we'll explore how to understand stack traces.


# (8c) Moving toward expert style

  (Recommended) Beginner code form:

  		RDD.transformation1()
		RDD.action1()
		RDD.transformation2()
		RDD.action2()

   Advanced code form:

   	    	RDD.transformation1().transformation2().action()


# (8d) Readability and code style: " ( one command per line ) "

       		(sc
		.parallelize(data)
		.map(lambda y: y - 1)
		.filter(lambda x: x < 10)
		.collect())



################################################################################
# Annex) To do section
################################################################################




################################################################################
# References
################################################################################

[1] Spark Documentation - spark.apache.org/documentation.html

[2] Spark Programming Guide -spark.apache.org/docs/latest/programming-guide.html

[3] Spark Python API Docs - spark.apache.org/docs/latest/api/python/

[4] How-to: Tune Your Apache Spark Jobs - http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/

